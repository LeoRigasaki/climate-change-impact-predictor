#!/usr/bin/env python3
"""
ğŸŒ Global Climate Data Processing Pipeline - Day 4 Enhanced Demo
tools/run_pipeline.py

Demonstrates the complete adaptive pipeline with global location support.
Shows off Day 4 achievements: any location on Earth with intelligent adaptation.
"""

import sys
import logging
import argparse
from datetime import datetime, timedelta
from pathlib import Path

# Add src to path
sys.path.append(str(Path(__file__).parent.parent / "src"))

from src.core.pipeline import ClimateDataPipeline
from src.core.location_service import LocationService

logging.basicConfig(level=logging.INFO)

async def main():
    """Enhanced main function demonstrating Day 4 global capabilities."""
    
    print("ğŸŒ Day 4 Enhanced: Global Climate Data Processing Pipeline")
    print("=" * 65)
    print("âœ¨ Now supports ANY location on Earth with adaptive intelligence!")
    print(f"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Global Climate Pipeline Demo")
    parser.add_argument("--location", "-l", type=str, 
                       help="Location to process (e.g., 'Tokyo, Japan', '40.7128,-74.0060')")
    parser.add_argument("--demo-mode", action="store_true",
                       help="Run demo with multiple global locations")
    parser.add_argument("--collect", action="store_true",
                       help="Force fresh data collection (slower but shows full capabilities)")
    parser.add_argument("--quick", action="store_true",
                       help="Use last 30 days instead of full range")
    
    args = parser.parse_args()
    
    # Initialize services
    pipeline = ClimateDataPipeline()
    location_service = LocationService()
    
    # Set date range
    end_date = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
    if args.quick:
        start_date = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
        print("âš¡ Quick mode: Using last 30 days of data")
    else:
        start_date = (datetime.now() - timedelta(days=365)).strftime("%Y-%m-%d")
        print("ğŸ“… Full mode: Using last year of data")
    
    print(f"ğŸ“… Date range: {start_date} to {end_date}")
    print()
    
    # Handle different modes
    if args.demo_mode:
        await run_global_demo(pipeline, location_service, start_date, end_date, args.collect)
    elif args.location:
        await run_single_location(pipeline, location_service, args.location, start_date, end_date, args.collect)
    else:
        await run_interactive_mode(pipeline, location_service, start_date, end_date, args.collect)

async def run_single_location(pipeline, location_service, location_input, start_date, end_date, force_collect=False):
    """Process a single global location."""
    
    print(f"ğŸ¯ Single Location Mode: Processing '{location_input}'")
    print("-" * 50)
    
    # Step 1: Resolve location
    print(f"ğŸ” Resolving location: '{location_input}'")
    
    try:
        # Check if input looks like coordinates
        if ',' in location_input and all(
            part.replace('.', '').replace('-', '').isdigit() 
            for part in location_input.split(',')
        ):
            # Parse coordinates directly
            lat_str, lon_str = location_input.split(',')
            latitude, longitude = float(lat_str.strip()), float(lon_str.strip())
            location_info = location_service.reverse_geocode(latitude, longitude)
            
            if not location_info:
                print(f"âŒ Could not resolve coordinates {latitude}, {longitude}")
                return
            
            print(f"ğŸ“ Coordinates resolved to: {location_info.name}, {location_info.country}")
        else:
            # Geocode location string
            location_info = location_service.geocode_location(location_input)
            
            if not location_info:
                print(f"âŒ Could not find location: '{location_input}'")
                print("ğŸ’¡ Try formats like: 'Tokyo, Japan', 'New York, USA', '40.7128,-74.0060'")
                return
            
            print(f"âœ… Location found: {location_info.name}, {location_info.country}")
            print(f"ğŸ“ Coordinates: {location_info.latitude:.4f}, {location_info.longitude:.4f}")
    
    except Exception as e:
        print(f"âŒ Location resolution failed: {e}")
        return
    
    print()
    
    # Step 2: Run global processing pipeline
    await process_location_with_pipeline(pipeline, location_service, location_info, start_date, end_date, force_collect=force_collect)

def run_global_demo(pipeline, location_service, start_date, end_date, force_collect=False):
    """Run demo with multiple diverse global locations."""
    
    print("ğŸŒ Global Demo Mode: Processing Diverse Locations Worldwide")
    print("=" * 60)
    print("ğŸ¯ Showcasing adaptive intelligence across different regions")
    print()
    
    # Diverse demo locations across continents and climates
    demo_locations = [
        "Berlin, Germany",       # Europe - Temperate
        "Tokyo, Japan",          # Asia - Humid subtropical
        "Reykjavik, Iceland",    # Arctic - Sub-arctic
        "Dubai, UAE",            # Middle East - Desert
        "Sydney, Australia",     # Oceania - Oceanic
        "SÃ£o Paulo, Brazil"      # South America - Subtropical
    ]
    
    successful_locations = 0
    total_features_processed = 0
    
    print(f"ğŸ“ Processing {len(demo_locations)} locations globally:")
    for location_name in demo_locations:
        print(f"   â€¢ {location_name}")
    print()
    
    for i, location_name in enumerate(demo_locations, 1):
        print(f"ğŸŒ [{i}/{len(demo_locations)}] Processing: {location_name}")
        print("-" * 40)
        
        # Resolve location
        try:
            location_info = location_service.geocode_location(location_name)
            if not location_info:
                print(f"âŒ Could not resolve {location_name}")
                continue
            
            print(f"ğŸ“ Resolved: {location_info.name}, {location_info.country}")
            print(f"ğŸ“ Coordinates: {location_info.latitude:.4f}, {location_info.longitude:.4f}")
            
            # Process with pipeline
            success, features = process_location_with_pipeline(
                pipeline, location_service, location_info, start_date, end_date, summary_mode=True, force_collect=force_collect
            )
            
            if success:
                successful_locations += 1
                total_features_processed += features
                print(f"âœ… Success: {features} features processed")
            else:
                print("âŒ Processing failed")
            
        except Exception as e:
            print(f"âŒ Failed to process {location_name}: {e}")
        
        print()
    
    # Global demo summary
    print("ğŸ¯ Global Demo Summary:")
    print("=" * 30)
    print(f"âœ… Successful Locations: {successful_locations}/{len(demo_locations)}")
    print(f"ğŸ“Š Total Features Processed: {total_features_processed}")
    print(f"ğŸŒ Global Coverage Demonstrated: {successful_locations/len(demo_locations)*100:.1f}%")
    
    if successful_locations > 0:
        avg_features = total_features_processed / successful_locations
        print(f"ğŸ“ˆ Average Features per Location: {avg_features:.1f}")
        print("\nğŸ‰ Day 4 Global Pipeline: SUCCESS!")
        print("ğŸŒ Adaptive intelligence working across diverse global regions!")
    else:
        print("\nâš ï¸ No locations processed successfully")

async def run_interactive_mode(pipeline, location_service, start_date, end_date, force_collect=False):
    """Run interactive mode for user input."""
    
    print("ğŸ¤– Interactive Mode: Global Location Processing")
    print("=" * 45)
    print("ğŸŒ Enter any location on Earth for climate data processing!")
    print()
    
    # Show examples
    print("ğŸ’¡ Examples:")
    print("   â€¢ City: 'Paris, France'")
    print("   â€¢ Landmark: 'Mount Everest'")
    print("   â€¢ Coordinates: '35.6762, 139.6503'")
    print("   â€¢ Remote area: 'McMurdo Station, Antarctica'")
    print()
    print("ğŸ’¡ Tip: Use --collect flag to force fresh data collection")
    print("   Example: python tools/run_pipeline.py --location 'Tokyo' --collect")
    print()
    
    while True:
        location_input = input("ğŸŒ Enter location (or 'quit' to exit): ").strip()
        
        if location_input.lower() in ['quit', 'exit', 'q']:
            print("ğŸ‘‹ Goodbye!")
            break
        
        if not location_input:
            print("âŒ Please enter a location.")
            continue
        
        print()
        await run_single_location(pipeline, location_service, location_input, start_date, end_date, force_collect)
        print("\n" + "-" * 50)

async def process_location_with_pipeline(pipeline, location_service, location_info, start_date, end_date, summary_mode=False, force_collect=False):
    """Process a location using the enhanced global pipeline."""
    
    if not summary_mode:
        print(f"ğŸ”„ Running enhanced global pipeline for {location_info.name}")
        print("ğŸ¯ Day 4 Features: Adaptive collection + Intelligent processing")
        print()
    
    try:
        # If force_collect is True, skip checking existing data
        if force_collect:
            if not summary_mode:
                print("ğŸ“¥ Collecting fresh data (forced)...")
                print("â±ï¸ This may take 30-60 seconds...")
                print()
            
            results = await pipeline.process_global_location(
                location=location_info,
                start_date=start_date,
                end_date=end_date,
                skip_collection=False  # Force data collection
            )
        else:
            # First, try with existing data
            if not summary_mode:
                print("ğŸ” Step 1: Checking for existing data...")
            
            results = await pipeline.process_global_location(
                location=location_info,
                start_date=start_date,
                end_date=end_date,
                skip_collection=True  # Try existing data first
            )
            
            # Check if we got any successful results
            successful_sources = len([r for r in results.values() 
                                    if r and r != results.get('_metadata') and 'data' in r])
            
            # If no existing data found, collect new data
            if successful_sources == 0 and not summary_mode:
                print("ğŸ“¥ No existing data found. Collecting fresh data...")
                print("â±ï¸ This may take 30-60 seconds...")
                print()
                
                # Collect new data
                results = await pipeline.process_global_location(
                    location=location_info,
                    start_date=start_date,
                    end_date=end_date,
                    skip_collection=False  # Actually collect data
                )
            elif successful_sources == 0 and summary_mode:
                # In demo mode, try a quick collection with shorter date range
                quick_start = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")
                results = await pipeline.process_global_location(
                    location=location_info,
                    start_date=quick_start,
                    end_date=end_date,
                    skip_collection=False
                )
        
        if summary_mode:
            # Return success status and feature count for demo mode
            successful_sources = len([r for r in results.values() 
                                    if r and r != results.get('_metadata') and 'data' in r])
            total_features = 0
            
            for source, result in results.items():
                if source != '_metadata' and result and 'data' in result:
                    data = result['data']
                    if hasattr(data, 'shape') and len(data.shape) > 1:
                        total_features += data.shape[1]
                    elif hasattr(data, 'columns'):
                        total_features += len(data.columns)
            
            return successful_sources > 0, total_features
        
        # Full detailed output
        print("ğŸ“Š Enhanced Processing Results:")
        print("=" * 40)
        
        # Processing metadata
        if '_metadata' in results:
            metadata = results['_metadata']
            print(f"ğŸ¯ Processing Summary:")
            print(f"   Sources Attempted: {metadata['data_sources_attempted']}")
            print(f"   Sources Successful: {metadata['data_sources_successful']}")
            print(f"   Available Sources: {', '.join(metadata.get('available_sources', []))}")
            print(f"   Processing Time: {metadata['processing_timestamp']}")
            
            if metadata.get('processing_errors'):
                print(f"   âš ï¸  Errors: {len(metadata['processing_errors'])}")
            print()
        
        total_features = 0
        total_records = 0
        successful_sources = 0
        
        # Individual source results
        for source, result in results.items():
            if source == '_metadata':
                continue
                
            if result and 'data' in result:
                successful_sources += 1
                data = result['data']
                quality_report = result.get('quality_report', {})
                quality_score = quality_report.get('overall_score', 0)
                
                print(f"âœ… {source.upper()}:")
                
                # Handle different data types
                if hasattr(data, 'shape'):  # DataFrame
                    records = data.shape[0]
                    features = data.shape[1] if len(data.shape) > 1 else 1
                    print(f"   Records: {records}")
                    print(f"   Features: {features}")
                    
                    if source == 'integrated':
                        total_features = features
                        total_records = records
                    
                    if hasattr(data, 'index'):
                        print(f"   Date Range: {data.index.min()} to {data.index.max()}")
                    
                elif isinstance(data, (list, tuple)):
                    records = len(data)
                    print(f"   Records: {records}")
                    print(f"   Type: {type(data).__name__}")
                else:
                    print(f"   Type: {type(data).__name__}")
                
                print(f"   Quality Score: {quality_score:.1f}/100")
                print()
                
            else:
                print(f"âŒ {source.upper()}: Processing failed or no data")
                print()
        
        # Enhanced summary with Day 4 features
        print("ğŸ¯ Enhanced Pipeline Summary:")
        print("=" * 35)
        
        if successful_sources > 0:
            success_rate = (successful_sources / len([k for k in results.keys() if k != '_metadata'])) * 100
            print(f"ğŸ“ˆ Success Rate: {success_rate:.1f}% ({successful_sources} sources)")
            print(f"ğŸŒ Location: {location_info.name}, {location_info.country}")
            print(f"ğŸ“Š Global Processing: âœ… WORKING")
            print(f"ğŸ¯ Adaptive Collection: âœ… WORKING")
            
            if total_features > 0:
                print(f"ğŸ“Š Total Features: {total_features}")
                print(f"ğŸ“ˆ Total Records: {total_records}")
            
            print(f"ğŸ’¾ Data Saved: data/processed/ & data/raw/")
            print()
            print("ğŸ‰ SUCCESS: Day 4 Global Pipeline Operational!")
            print("âœ¨ Adaptive intelligence working for this location!")
            print("ğŸŒ Ready for: Any location on Earth!")
            
        else:
            print("âš ï¸ No data sources processed successfully")
            print(f"ğŸ“ Location: {location_info.name} may have limited data coverage")
            print("ğŸ’¡ This demonstrates adaptive graceful degradation!")
        
        return True, total_features
        
    except Exception as e:
        print(f"âŒ Pipeline processing failed: {e}")
        if not summary_mode:
            print("ğŸ’¡ This might indicate:")
            print("   â€¢ Network connectivity issues")
            print("   â€¢ API rate limiting")
            print("   â€¢ Regional data limitations (expected behavior)")
        return False, 0

def show_day4_achievements():
    """Show Day 4 achievements summary."""
    
    print("\nğŸ‰ Day 4 Enhanced Global Pipeline Achievements:")
    print("=" * 50)
    print("âœ… Global Location Support - Any coordinates on Earth")
    print("âœ… Adaptive Data Collection - Smart regional adaptation")
    print("âœ… Intelligent Processing - Handles data availability gracefully")
    print("âœ… Location-Aware Caching - Performance optimized")
    print("âœ… Enhanced Error Handling - Graceful degradation")
    print("âœ… Multi-Source Integration - Combines available data sources")
    print("âœ… Production-Ready Architecture - Interview-ready code quality")
    print()
    print("ğŸŒ From static analysis â†’ Dynamic global climate prediction platform!")
    print()
    print("ğŸ’¡ Pro tip: Use --collect flag to see full data collection in action:")
    print("   python tools/run_pipeline.py --location 'Tokyo, Japan' --collect")

if __name__ == "__main__":
    try:
        import asyncio
        asyncio.run(main())
        show_day4_achievements()
        print(f"\nCompleted: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Pipeline demo interrupted by user. Goodbye!")
    except Exception as e:
        print(f"\nâŒ Demo failed with error: {e}")
        print("ğŸ’¡ Make sure you have:")
        print("   â€¢ Internet connection for location services")
        print("   â€¢ Required dependencies installed")
        print("   â€¢ Some existing data in data/raw/ directory")